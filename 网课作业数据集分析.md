## 网课作业数据集分析

实现编程语言：python 3.8

中文分词框架：jieba（可在github上下载）

代码：https://github.com/DraftTin/AprioriAlgorithm.git

1. **简单介绍：**使用Apriori算法对答案中的频繁项和关联模式进行分析，代码量较少（只有200+行），这里简单介绍下我完成的Apriori算法执行步骤

   ```python
   # processApriori: apriori算法计算频繁项集和关联规则
   # - 获取所有单个项, 获取所有事务的列表
   # - 根据当前的项集生成相应的频繁项集, 保存
   # - 生成 k + 1 元项集, 保存, 循环执行直到生成的频繁项集为空
   # - 遍历所有频繁项集的所有项
   # - 将每一项拆分成两个子集, 计算置信度, 将满足置信度的分割方式记录, 保存
   # - 返回所有频繁项, 所有保存的关联规则和相应的支持度和置信度
   ```

   可以看到，算法实现还是很简单的，缺点是复杂度有点高，但由于数据量不大，所以只要对数据的初始化做的不错，运行时间还是很短的（5s左右）。

2. **频繁项和关联模式的挖掘：**

   （1）**第一题：**第一题的答案很长，内容也很杂，所以数据的预处理很重要，否则算法效率很低然后结果也会有很多无关的东西，所以这里说下预处理过程，首先对每项按照词性进行分词，只保留名词和动词（其他虚词没用），但其实动词里面也有很多没用的东西，如'进行'，'产生'，'存在'这种会出现很多次但其实很没用的词，所以需要提前设计一个集合用于排除这类的词，最终才能得到我们想要的结果。

   **挖掘结果：**挖掘结果体现在两个文件中rule.txt和item.txt，分别保存关联模式和频繁项，由于文本量很大，所以挖掘出了很多关系，这里我说几个比较有代表性的一对一的**关联关系**(itemA -> itemB : confidence)，所有的关系会在提交的文件中给出：

   | itemA            | itemB          | confidence | 评价                         |
   | ---------------- | -------------- | ---------- | ---------------------------- |
   | 促销             | 购物券         | 0.918919   | 正常                         |
   | 网站，客户       | 个性化         | 1.000000   | 客户需要个性化服务           |
   | 半导体           | 利润，提高质量 | 0.787234   | 半导体很赚钱？               |
   | 制造业           | 道德风险       | 0.725490   | emmm......                   |
   | 销售额           | 打折、手段     | 0.945946   | 商家过的好不好都要看手段     |
   | 商品，避免       | 成本           | 0.969697   | 商品，成本，嗯               |
   | 保险公司，制造业 | 数据           | 0.975610   | 行业都需要数据               |
   | 识别             | 个性化、模式   | 0.660714   | 大概说的是识别用户的行为模式 |
   | 道德风险，利润   | 提高质量       | 0.649123   | 确实需要权衡                 |

   **频繁项集：**频繁项集出现的就表示和问题的相关程度，问题是"数据挖掘的应用"，所以频繁项很可能就和数据挖掘的应用有关，这里列出几个频繁项，可以看到，确实如此，具体频繁项在文件中可以看到。

   | item     | support  |
   | -------- | -------- |
   | 推荐     | 0.154605 |
   | 人脸识别 | 0.111842 |
   | 利润     | 0.197368 |
   | 商场     | 0.128289 |
   | 数据     | 0.404605 |
   | 电子商务 | 0.144737 |
   | 道德风险 | 0.187500 |
   | 制造业   | 0.167763 |
   | 保险公司 | 0.223684 |

   

   （2）**第二题：**第二题由于问题针对性比较强，所以模式就很明显了，挖掘出来的信息也较少。

   **关联模式：**

   | itemA            | itemB | confidence | 评价                                                         |
   | ---------------- | ----- | ---------- | ------------------------------------------------------------ |
   | 离散，异众，系数 | 比率  | 0.959184   |                                                              |
   | 异众，比率，系数 | 离散  | 0.886792   |                                                              |
   | 离散，比率，系数 | 异众  | 0.810345   | 可以看到离散，比率，系数，异众这四个一起出现的特别频繁，说明百度可能把它们放到一起了（狗头.jpg） |
   | 偏度             | 峰度  | 0.944828   |                                                              |
   | 峰度             | 偏度  | 0.931973   | 这两个也是同理                                               |

   置信度高的组合很可能是因为百度搜索结果中放到一起的较多。

   **频繁项：**

   | item                   | support  |
   | ---------------------- | -------- |
   | 峰度，偏度             | 0.450658 |
   | 离散，异众，系数，比率 | 0.154605 |
   | 峰度，系数，偏度       | 0.266447 |
   | 峰度                   | 0.483553 |
   | 系数                   | 0.595395 |
   | 偏度                   | 0.476974 |

   可以看到：峰度，偏度，系数答的人最多，而在答题的人中同时答"离散，异众，系数，比率"

   几个的又很多。

3. **抄袭现象是否存在？**

   网课作业，抄袭现象是肯定存在的，但我们要如何使用高大上的数据挖掘发现这种抄袭现象呢？首先想到的肯定就是字符串匹配了，确实，在判断两个答案间是否有抄袭现象字符串匹配是必须的，但是我们又不能盲目地将所有答案成对匹配（复杂度太高），怎么找呢？这里使用的方法是先从高频项中选出若干项（最好是项中含有词较多的），然后对每个含有项中所有词的答案进行匹配（两两匹配），如果匹配度超过thresh，那么就认为是抄袭。

   ```python
   # findPlagiarisms: 查找抄袭现象函数, 返回被认为是抄袭的元组对的列表
   # - 找到包含所有包含高频项中所有词的答案, 保存
   # - 将保存的这些可疑的答案进行模糊匹配, 如果匹配度超过阙值则认为是抄袭
   def findPlagiarisms(data, rItems, plagiarismThresh):
       # 获取若干高频项
       highFrequencyItems = findItemForPlagiarisms(rItems)
       plagiarisms = set()
       for item in highFrequencyItems:
           # 使用高频项查找抄袭现象
           tmp = findPlagiarismsFromItem(data, item, plagiarismThresh)
           plagiarisms = plagiarisms.union(tmp)
       return plagiarisms
   ```

   这里列出几个可能互相抄袭的组(其实有很多匹配的，这里不一一列出，会在文件中给出）：

   ```
   第一题:
   (121, 222)
   (201, 230)
   (207, 209)
   (218, 209)
   (233, 228)
   (207, 218)
   (121, 261)
   (222, 261)
   (148, 232)
   (209, 218)
   ....
   第二题: 
   (96, 104)
   (12, 251)
   (104, 96)
   (88, 109)
   (94, 12)
   (105, 79)
   (92, 79)
   (41, 314)
   (105, 317)
   (191, 82)
   (301, 82)
   ```

   这里说下：高频词集的选择是比较自由的，模糊匹配算法也是比较灵活的